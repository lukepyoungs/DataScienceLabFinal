{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions\n",
        "!pip install word2number\n",
        "!pip install Unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_2mfS8fYgYI",
        "outputId": "c87bb7a1-3bf1-4dd6-d3fd-98e117785c66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "  Downloading pyahocorasick-2.0.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting word2number\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: word2number\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5582 sha256=22351e6eed56e4fd9854638a6ba2f5f5757e963995a05a7f9a252dc9df166120\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/4a/5b/d2f2df5c344ddbecb8bea759872c207ea91d93f57fb54e816e\n",
            "Successfully built word2number\n",
            "Installing collected packages: word2number\n",
            "Successfully installed word2number-1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Unidecode\n",
            "Successfully installed Unidecode-1.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Offensive Text Detector"
      ],
      "metadata": {
        "id": "HQS4qvPrmvhJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Useful functions"
      ],
      "metadata": {
        "id": "5yp5zuNFn10Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "from sklearn.feature_extraction.text import CountVectorizer # bag-of-words\n",
        "import offensive_preprocessing as preprocess\n",
        "\n",
        "\"\"\" extract dataframe from dataset on file \"\"\"\n",
        "def offensiveDataframe(filename: str, include_bag_of_words: bool, include_bad_words: bool, include_slur: bool):\n",
        "\n",
        "    offensive_feature_dict = {\"text\": []}\n",
        "    if(include_bad_words):\n",
        "        offensive_feature_dict[\"has_badword\"] = []\n",
        "    if(include_slur):\n",
        "        offensive_feature_dict[\"has_slur\"] = []\n",
        "    offensive_feature_dict[\"is_offensive\"] = []\n",
        "\n",
        "    offensive_df = pd.DataFrame(offensive_feature_dict)\n",
        "    print(offensive_df)\n",
        "    offensive_arr = []\n",
        "\n",
        "    # Open file for reading\n",
        "    with open(filename, \"r\", encoding='utf-8-sig') as file_ptr:\n",
        "\n",
        "        # Make badword/slur dictionaries\n",
        "        f_bad = open('./content/Bad_Word_Data.txt')\n",
        "        bad = preprocess.make_dict(f_bad)\n",
        "\n",
        "        \n",
        "        f_slur = open('./content/Slur_Data.txt')\n",
        "        slur = preprocess.make_dict(f_slur)\n",
        "\n",
        "        # Read each line of the file\n",
        "        file_csv = csv.reader(file_ptr)\n",
        "        for line in file_csv:\n",
        "\n",
        "            ## get features\n",
        "\n",
        "            # text\n",
        "            offensive_line = line[0]\n",
        "\n",
        "            # contains badwords or slurs\n",
        "            processed = preprocess.text_preprocessing(offensive_line)\n",
        "            # print(\" \".join(processed))\n",
        "            offensive_arr.append(\" \".join(processed)) # add to bag-of-words\n",
        "\n",
        "            has_badword = 0 # badword\n",
        "            for token in processed:\n",
        "                if bad.get(token) == 1:\n",
        "                    has_badword = 1\n",
        "                    break\n",
        "\n",
        "            has_slur = 0    # slur\n",
        "            for token in processed:\n",
        "                if slur.get(token) == 1:\n",
        "                    has_slur = 1\n",
        "                    break\n",
        "\n",
        "            # is it offensive or not?\n",
        "            offensive = line[1]\n",
        "\n",
        "            ## add features to dataframe\n",
        "            # print(offensive_line, has_slur, has_badword, offensive)\n",
        "            _offensive_append(offensive_df, [offensive_line, has_slur, has_badword, offensive])\n",
        "\n",
        "    # Make a dataframe from extra \"bag-of-word\" features\n",
        "    if(include_bag_of_words):\n",
        "        bow_extracted = _get_bagOWords(offensive_arr)\n",
        "        bow_labels = bow_extracted[0]\n",
        "        bow_elements = bow_extracted[1]\n",
        "        bow_features = pd.DataFrame(np.array(bow_elements), columns=bow_labels)\n",
        "\n",
        "        offensive_df = pd.concat((offensive_df, bow_features), axis=1)\n",
        "\n",
        "        return offensive_df, bow_labels\n",
        "    \n",
        "    return offensive_df\n",
        "\n",
        "\"\"\" extract dataframe from given user string. \"\"\"\n",
        "def user_offensive_text(user_input: str, user_offensive_labels: list, include_bag_of_words: bool, include_bad_words: bool, include_slur: bool):\n",
        "\n",
        "    offensive_feature_dict = {\"text\": []}\n",
        "    if(include_bad_words):\n",
        "        offensive_feature_dict[\"has_badword\"] = []\n",
        "    if(include_slur):\n",
        "        offensive_feature_dict[\"has_slur\"] = []\n",
        "\n",
        "    # combine current labels and bag-of-words labels\n",
        "    offensive_df = pd.DataFrame(offensive_feature_dict)\n",
        "    # offensive_arr = []\n",
        "\n",
        "    # Make badword/slur dictionaries\n",
        "    f_bad = open('./content/Bad_Word_Data.txt')\n",
        "    bad = preprocess.make_dict(f_bad)\n",
        "\n",
        "    \n",
        "    f_slur = open('./content/Slur_Data.txt')\n",
        "    slur = preprocess.make_dict(f_slur)\n",
        "\n",
        "    ## get features\n",
        "\n",
        "    # contains badwords or slurs\n",
        "    processed = preprocess.text_preprocessing(user_input)\n",
        "    # offensive_arr.append(\" \".join(processed)) # add to bag-of-words\n",
        "\n",
        "    has_badword = 0 # badword\n",
        "    for token in processed:\n",
        "        if bad.get(token) == 1:\n",
        "            has_badword = 1\n",
        "            break\n",
        "\n",
        "    has_slur = 0    # slur\n",
        "    for token in processed:\n",
        "        if slur.get(token) == 1:\n",
        "            has_slur = 1\n",
        "            break\n",
        "\n",
        "    ## add features to dataframe\n",
        "    # print(offensive_line, has_slur, has_badword)\n",
        "    _offensive_append(offensive_df, [user_input, has_slur, has_badword])\n",
        "\n",
        "    if(include_bag_of_words):\n",
        "\n",
        "        # Make a dataframe from extra \"bag-of-word\" features\n",
        "        bow_extracted = _get_bagOWords([\" \".join(processed)])\n",
        "        bow_labels = bow_extracted[0]\n",
        "        # bow_elements = bow_extracted[1]\n",
        "\n",
        "        # print(user_offensive_labels)\n",
        "        # print(bow_labels)\n",
        "        user_offensive_features = []\n",
        "        for lbl in user_offensive_labels:\n",
        "            if(lbl in bow_labels):\n",
        "                user_offensive_features.append(1)\n",
        "            else:\n",
        "                user_offensive_features.append(0)\n",
        "        \n",
        "        # print(user_offensive_labels)\n",
        "        # print(user_offensive_features) # add this to a dataframe\n",
        "\n",
        "        user_offensive_df = pd.DataFrame([user_offensive_features], columns=np.array(user_offensive_labels))\n",
        "        return pd.concat((offensive_df, user_offensive_df), axis=1)\n",
        "    \n",
        "    return offensive_df\n",
        "\n",
        "\"\"\" create data features. \"\"\"\n",
        "def _get_bagOWords(data: list):\n",
        "    vector = CountVectorizer()\n",
        "\n",
        "    # Fit the bag-of-words model\n",
        "    bag_of_words = vector.fit_transform(data)\n",
        "\n",
        "    return (vector.get_feature_names_out(), bag_of_words.toarray())\n",
        "\n",
        "\"\"\" append offensive element to list \"\"\"\n",
        "def _offensive_append(offensive_df, features: list):\n",
        "    offensive_df.loc[len(offensive_df.index)] = features"
      ],
      "metadata": {
        "id": "u43VtArjmzlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing and Training"
      ],
      "metadata": {
        "id": "VoqxMC6pnlO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import offensive_parser as parser\n",
        "\n",
        "# offensive_df, bag_of_words = parser.offensiveDataframe(\"./content/Offensive_Dataset_Team.csv\", False, True, True)\n",
        "offensive_df = parser.offensiveDataframe(\"./content/Offensive_Dataset_Team.csv\", False, True, True)\n",
        "print(offensive_df.head())"
      ],
      "metadata": {
        "id": "ygaQm7jjm9DP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Split\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y = offensive_df['is_offensive']\n",
        "x = offensive_df.drop(['is_offensive', 'text'], axis = 1)\n",
        "\n",
        "# Train Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "gCz-JIHbm9v7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "thIMT5V8nc76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "print(\"Results for Logistic Regression on Bad Words and Slurs Words:\\n\")\n",
        "\n",
        "# train model\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# make predictions\n",
        "y_pred = lr.predict(X_test)\n",
        "pred = lr.predict_proba(X_test)\n",
        "\n",
        "train_accuracy = lr.score(X_train, y_train)\n",
        "print(\"Accuracy on train = %0.4f \" % train_accuracy)\n",
        "\n",
        "test_accuracy = lr.score(X_test, y_test)\n",
        "print(\"Accuracy on test = %0.4f \" % test_accuracy, \"\\n\")\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "eM3ptvgBnBQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iO0wVFF8nEYD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}